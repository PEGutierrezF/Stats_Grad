---
title: "Simple Linear Regresion"
subtitle: ""
author: "Pablo E. Guti\u00E9rrez-Fonseca"
institute: ""
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "css/nhsr.css", "css/nhsr-fonts.css", "css/custom.css"]
    nature:
      highlightLanguage: r
      highlightStyle: github
      highlightLines: true
      highlightSpans: true 
      countIncrementalSlides: true
      ratio: "16:9"
    includes:
      after_body: [css/insert-logo.html]
xaringanExtra:
    use_panelset: true
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(readxl)
library(readr)
library(lessR)
library(ggplot2)
library(patchwork)
library(palmerpenguins)
library(car)
library(ggforce) # for geom_circle
library(RVAideMemoire) #shapiro.test
knitr::opts_chunk$set(dpi= 300)
xaringanExtra::use_panelset()
```


# What is regression?
- Regression analysis is a generic term for a group of different statistical techniques. 

  1. The purpose of all these techniques is to examine the relationship between variables. 
      - Regression aims to find the **best-fitting linear equation ( $Y = \beta_0 + \beta_1 X_1 + \epsilon$ )** that describes the relationship between the dependent variable (often denoted as $Y$ ) and independent variables (denoted as $X1, X2, ..., Xn$ ).  
<br>
  2. The most common type of linear regression is Type I regression, in which we attempt to determine the relationship between dependent and explanatory or independent variables.

---
# What can we accomplish with linear regression?

.pull-left[
- Describing the nature of the relationship between two variables.

]


.pull-right[

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=5}
# Sample data
set.seed(14)
x <- seq(0, 22, by = 2)
y <- 0.3 * x + rnorm(length(x), mean = 0, sd = 0.5)  # Generate some example data with noise
df <- data.frame(x = x, y = y)

# Fit linear model
model <- lm(y ~ x, data = df)
intercept <- coef(model)[1]
slope <- coef(model)[2]

# Create ggplot with linear model fit to data and equation
p <- ggplot(data = df, aes(x = x, y = y)) +
  geom_smooth(method = "lm", color = "black", se=F, linewidth=0.8) +
  geom_point(color = "red", size = 1.5) +
  scale_y_continuous(limits = c(0, 10), name = "Dependent Variable (Y)") +
  scale_x_continuous(limits = c(0, 30), name = "Independent Variable (X)") +
  annotate("text", x = 5, y = 8, label = paste0("Y = ", round(intercept, 2), " + ", round(slope, 2), "X"), color = "blue", size = 4) +
  theme_bw()

p

```
]


---
# What can we accomplish with linear regression?

.pull-left[
- Describing the nature of the relationship between two variables.

- Predicting the dependent variable within the range of observed values. Aim to forecast y-values within the range of observed x-values. Offers relatively more certainty due to reliance on known data.

]


.pull-right[

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=5}
# Sample data
set.seed(14)
x <- seq(0, 22, by = 2)
y <- 0.3 * x + rnorm(length(x), mean = 0, sd = 0.5)  # Generate some example data with noise
df <- data.frame(x = x, y = y)

# Initial ggplot with linear model fit to data
p <- ggplot(data = df, aes(x = x, y = y)) +
  geom_smooth(method = "lm", color = "black", se=T, linewidth=0.8) +
  geom_point(color = "red", size = 1.5) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 10), name = "Dependent Variable (Y)") +
  scale_x_continuous(limits = c(0, 30), name = "Independent Variable (X)")

# Fit a linear model to the data
model <- lm(y ~ x, data = df)

# Choose a random x value within the extrapolated range (e.g., x = 25)
random_x <- 15
random_y <- predict(model, newdata = data.frame(x = random_x))

p + geom_segment(aes(x = random_x, xend = random_x, y = 0, yend = random_y), color = "red", linetype = "dashed") +  # Vertical line stopping at the fitted line
  geom_segment(aes(x = 0, xend = random_x, y = random_y, yend = random_y), color = "red", linetype = "dashed") +  # Horizontal line stopping at the fitted line
  annotate("text", x = 10, y = random_y + 0.5, label = paste("Predicted y =", round(random_y, 2)), color = "red")

```
]


---
# What can we accomplish with linear regression?
.pull-left[
- Describing the nature of the relationship between two variables.

- Predicting the dependent variable within the range of observed values. Aim to forecast y-values within the range of observed x-values. Offers relatively more certainty due to reliance on known data.

- Predicting the dependent variable beyond the range of observed values. 
    - Represents an exploration beyond the limits of observed data.
    - Naturally, this type of objective carries the greatest load of restrictions, assumptions, caveats, and risk of error.

]


.pull-right[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=5}
# Sample data
set.seed(14)
x <- seq(0, 22, by = 2)
y <- 0.3 * x + rnorm(length(x), mean = 0, sd = 0.5)  # Generate some example data with noise
df <- data.frame(x = x, y = y)

# Initial ggplot with linear model fit to data
p <- ggplot(data = df, aes(x = x, y = y)) +
  geom_smooth(method = "lm", color = "black", se=T, linewidth=0.8) +
  geom_point(color = "red", size = 1.5) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 10), name = "Dependent Variable (Y)") +
  scale_x_continuous(limits = c(0, 30), name = "Independent Variable (X)")

# Fit a linear model to the data
model <- lm(y ~ x, data = df)

# Create new data frame for extrapolation from x = 22 to 30
new_x <- data.frame(x = seq(22, 30, by = 1))
predictions <- predict(model, newdata = new_x, interval = "confidence", level = 0.95)
newdf <- cbind(new_x, predictions)

# Add extrapolated line and confidence ribbon to the plot
p + 
 # geom_ribbon(data = newdf, aes(x = x, ymin = lwr, ymax = upr), fill = "grey90") +
  geom_line(data = newdf, aes(x = x, y = fit), color = 'steelblue', lwd = 1.2, lty = 2) +
  ggtitle("")

```
]

---
# Difference Between Correlation and Linear Simple Regression
1. Degree & Nature of Relationship:
    - Correlation is a measure of degree of relationship between X & Y.
    - Regression studies the nature of relationship between the variables so that one may be able to predict the value of one variable on the basis of another. 

---
# Difference Between Correlation and Linear Simple Regression
2. Cause & Effect Relationship:
    - Correlation does **not imply a cause-and-effect relationship** between two variables.
    - Regression models a **cause-and-effect relationship**, where:
        - The independent variable is the cause.
        - The dependent variable is the effect.
 

---
# Difference Between Correlation and Linear Simple Regression
3. Independent and Dependent Relationship:
    - In correlation, there is **no distinction** between independent and dependent variables.
    - In regression, the model includes:
        - Slope ( $\beta_1$ ): the change in the dependent variable per unit increase in the independent variable.
        - Intercept ( $\beta_0$ ): the value of the dependent variable when the independent variable is zero.

---
# When to use a Simple Linear Regression
- You have two continuous variables measured in the same unit.
- You suspect the variables are related and vary in unison.
- You aim to predict one variable based on the other.
- You want to evaluate a trend (slope) over multiple measurements.

---
# Assumptions 
1. Independent, random samples
2. Linear relationship between variables
3. Normally distributed residuals (after fitting the model)

---
# Building blocks of a linear regression model


---
# Building blocks of a linear regression model
.pull-left[
- **Create a Predictive Equation**: 
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.

- **Evaluate the Model**: 
    - Quantify how well the line fits the data (e.g., using metrics like R2R2 or residual analysis).
    - Assess the model's predictive power and understand its limitations in making predictions beyond the observed data
]


---
# Building blocks of a linear regression model

.pull-left[
- **Create a Predictive Equation**:  
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

plot( Income ~ Education , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE, xlab="Fraction having attained a Bachelor's degree")

```
]


---
# Building blocks of a linear regression model

.pull-left[
- **Create a Predictive Equation**: 
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

plot( Income ~ Education , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE, xlab="Fraction having attained a Bachelor's degree")

text(0.30, 22500,  paste("Observed value")) 


```
]



---
# Building blocks of a linear regression model

.pull-left[
- **Create a Predictive Equation**: 
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")

text(0.30, 22500,  paste("Observed value")) 

```
]


---
# Building blocks of a linear regression model

.pull-left[
- **Create a Predictive Equation**: 
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")

text(0.30, 22500,  paste("Observed value")) 

text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

```
]


---
# Building blocks of a linear regression model

.pull-left[
- **Create a Predictive Equation**: 
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")

text(0.30, 22500,  paste("Observed value")) 

text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

```
]


---
# Building blocks of a linear regression model

.pull-left[
- **Create a Predictive Equation**: 
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")

text(0.30, 22500,  paste("Observed value")) 

text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

# Draw dashed lines for residuals
residuals <- M$residuals
fitted_values <- fitted(M)

# Loop through the data points to draw dashed lines from each data point to the fitted line
for (i in 1:length(residuals)) {
  segments(dat$Education[i], dat$Income[i], dat$Education[i], fitted_values[i], col = "red", lty = 2)
}


```
]



---
# Building blocks of a linear regression model

.pull-left[
- **Create a Predictive Equation**: 
    - Develop an equation for a line that most accurately predicts the response variable based on the explanatory variable.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")

text(0.30, 22500,  paste("Observed value")) 
text(0.284, 23700,  paste("Residuals"), col='red') 

text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

# Draw dashed lines for residuals
residuals <- M$residuals
fitted_values <- fitted(M)

# Loop through the data points to draw dashed lines from each data point to the fitted line
for (i in 1:length(residuals)) {
  segments(dat$Education[i], dat$Income[i], dat$Education[i], fitted_values[i], col = "red", lty = 2)
}


```
]

---
# Building blocks of a linear regression model

.pull-left[
- Calculate the model parameters (**intercept** and **slope**) for the line that cuts through the observations with the **least amount of error (squared error)**.  
  -   This is referred to as a **Least Squares Regression (LSR)**.

- LSR does this by minimizing the sum of the squares of the differences between the observed dependent variable values and those predicted by the linear model. 
    - These differences are called **residuals**.

]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")

text(0.30, 22500,  paste("Observed value")) 
text(0.284, 23700,  paste("Residuals"), col='red') 

text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

# Draw dashed lines for residuals
residuals <- M$residuals
fitted_values <- fitted(M)

# Loop through the data points to draw dashed lines from each data point to the fitted line
for (i in 1:length(residuals)) {
  segments(dat$Education[i], dat$Income[i], dat$Education[i], fitted_values[i], col = "red", lty = 2)
}


```
]

---
# Building blocks of a linear regression model

.pull-left[
- The sum of squared error in regression is:

- $\text{SSE} = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y - \hat{y})^2$

]




---
# Building blocks of a linear regression model

.pull-left[
- The sum of squared error in regression is:

- $\text{SSE} = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y - \hat{y})^2$
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# Sample data for illustration
x <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
       0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
       0.19, 0.28)
y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)
dat <- data.frame(Education = x, Income = y)

# Fit a linear model
M <- lm(Income ~ Education, data = dat)

# Plot all points and regression line
plot(Income ~ Education, data = dat, pch = 16, ylab="", 
     xlab="Fraction with Bachelor's Degree", las=1, bty="n", col="gray")
abline(M, col="blue")

# Pick a single point to highlight
i <- 16  # Index of observation to highlight
points(x[i], y[i], col="black", pch=16)  # Highlighted observed point
```
]

---
# Building blocks of a linear regression model

.pull-left[
- The sum of squared error in regression is:

- $\text{SSE} = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y - \hat{y})^2$
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# Sample data for illustration
x <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
       0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
       0.19, 0.28)
y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)
dat <- data.frame(Education = x, Income = y)

# Fit a linear model
M <- lm(Income ~ Education, data = dat)

# Plot all points and regression line
plot(Income ~ Education, data = dat, pch = 16, ylab="", 
     xlab="Fraction with Bachelor's Degree", las=1, bty="n", col="gray")
abline(M, col="blue")

# Pick a single point to highlight
i <- 16  # Index of observation to highlight
points(x[i], y[i], col="black", pch=16)  # Highlighted observed point

text(x[i], y[i]+0.009, expression(y[i]), pos=4, col="black", cex=1.7)

```
]


---
# Building blocks of a linear regression model

.pull-left[
- The sum of squared error in regression is:

- $\text{SSE} = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y - \hat{y})^2$
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# Sample data for illustration
x <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
       0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
       0.19, 0.28)
y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)
dat <- data.frame(Education = x, Income = y)

# Fit a linear model
M <- lm(Income ~ Education, data = dat)

# Plot all points and regression line
plot(Income ~ Education, data = dat, pch = 16, ylab="", 
     xlab="Fraction with Bachelor's Degree", las=1, bty="n", col="gray")
abline(M, col="blue")

# Pick a single point to highlight
i <- 16  # Index of observation to highlight
points(x[i], y[i], col="black", pch=16)  # Highlighted observed point

text(x[i], y[i]+0.009, expression(y[i]), pos=4, col="black", cex=1.7)

# Predicted y-value for the selected x-value
y_hat <- predict(M, newdata = data.frame(Education = x[i]))


# Add labels to indicate observation and predicted value
text(x[i], y_hat, expression(hat(y)[i]), pos=4, col="blue", cex=1.7)
text(0.02, y_hat, expression(paste("Predicted ", hat(y)[i])), pos=4, col="red")

# Annotate the plot with the regression equation
# equation_text <- expression(Y == beta[0] + beta[1] * X)
# text(0.30, 31200, equation_text, col="blue", cex=1.7)
```
]


---
# Building blocks of a linear regression model

.pull-left[
- The sum of squared error in regression is:

- $\text{SSE} = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y - \hat{y})^2$
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# Sample data for illustration
x <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
       0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
       0.19, 0.28)
y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)
dat <- data.frame(Education = x, Income = y)

# Fit a linear model
M <- lm(Income ~ Education, data = dat)

# Plot all points and regression line
plot(Income ~ Education, data = dat, pch = 16, ylab="", 
     xlab="Fraction with Bachelor's Degree", las=1, bty="n", col="gray")
abline(M, col="blue")

# Pick a single point to highlight
i <- 16  # Index of observation to highlight
points(x[i], y[i], col="black", pch=16)  # Highlighted observed point

text(x[i], y[i]+0.009, expression(y[i]), pos=4, col="black", cex=1.7)

# Predicted y-value for the selected x-value
y_hat <- predict(M, newdata = data.frame(Education = x[i]))

# Draw a line from observation to fitted line (residual)
segments(x[i], y[i], x[i], y_hat, col="red", lwd=1.5)

# Draw lines from the fitted point to both axes
segments(x[i], y_hat, 0, y_hat, col="red", lty=2)  # Horizontal line to y-axis
segments(x[i], 0, x[i], y_hat, col="red", lty=2)   # Vertical line to x-axis

# Add labels to indicate observation and predicted value
text(x[i], y_hat, expression(hat(y)[i]), pos=4, col="blue", cex=1.7)
text(0.02, y_hat, expression(paste("Predicted ", hat(y)[i])), pos=4, col="red")

# Annotate the plot with the regression equation
# equation_text <- expression(Y == beta[0] + beta[1] * X)
# text(0.30, 31200, equation_text, col="blue", cex=1.7)
```
]



---
# Building blocks of a linear regression model
.pull-left[
- The sum of squared error in regression is:

- $\text{SSE} = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y - \hat{y})^2$

]


.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.width=5, fig.height=6}
# Sample data for illustration
x <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
       0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
       0.19, 0.28)
y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)
dat <- data.frame(Education = x, Income = y)

# Fit a linear model
M <- lm(Income ~ Education, data = dat)

# Plot all points and regression line
plot(Income ~ Education, data = dat, pch = 16, ylab="", 
     xlab="Fraction with Bachelor's Degree", las=1, bty="n", col="gray")
abline(M, col="blue")

# Pick a single point to highlight
i <- 16  # Index of observation to highlight
points(x[i], y[i], col="black", pch=16)  # Highlighted observed point

text(x[i], y[i]+0.009, expression(y[i]), pos=4, col="black", cex=1.7)

# Predicted y-value for the selected x-value
y_hat <- predict(M, newdata = data.frame(Education = x[i]))

# Draw a line from observation to fitted line (residual)
segments(x[i], y[i], x[i], y_hat, col="blue", lwd=1.5)

# Draw lines from the fitted point to both axes
segments(x[i], y_hat, 0, y_hat, col="red", lty=2)  # Horizontal line to y-axis
segments(x[i], 0, x[i], y_hat, col="red", lty=2)   # Vertical line to x-axis

# Add labels to indicate observation and predicted value
text(x[i], y_hat, expression(hat(y)[i]), pos=4, col="blue", cex=1.7)
text(0.02, y_hat, expression(paste("Predicted ", hat(y)[i])), pos=4, col="red")

# Annotate the error term e_i = y_i - y_hat with a bracket
# Position the text near the middle of the residual line
middle_y <- (y[i] + y_hat) / 2  # Midpoint of the y-coordinates for the residual
text(x[i] + 0.03, middle_y, labels=expression(e[i] == y[i] - hat(y)[i]), col="blue", pos=4, cex=1.4)
text(x[i] + 0.009, middle_y, "}", srt=0, col="blue", cex=3)
```
]



---
# Building blocks of a linear regression model

.pull-left[
- **Best fit** in context of OLS (Ordinary Least Square, a type of Least Square) means that the sum of the squares of the residuals (or error SSE) us as **small as possible**.

- Mathematically, it's about finding the values of slope ( $\beta_1$, $\beta_2$, ., $\beta_n$ ) that minimize this sum.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='top', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")

text(0.30, 22500,  paste("Observed value")) 
text(0.284, 23700,  paste("Residuals"), col='red') 

text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

# Draw dashed lines for residuals
residuals <- M$residuals
fitted_values <- fitted(M)

# Loop through the data points to draw dashed lines from each data point to the fitted line
for (i in 1:length(residuals)) {
  segments(dat$Education[i], dat$Income[i], dat$Education[i], fitted_values[i], col = "red", lty = 2)
}


```
]


---
# Building blocks of a linear regression model

.pull-left[
- **Slopes**
]


---
# Building blocks of a linear regression model

.pull-left[
- **Slopes** ( $\beta_1$, $\beta_2$, ., $\beta_n$ ): These coefficients represent the change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='top', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")


text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

# Draw dashed lines for residuals
residuals <- M$residuals
fitted_values <- fitted(M)

# Loop through the data points to draw dashed lines from each data point to the fitted line
for (i in 1:length(residuals)) {
  segments(dat$Education[i], dat$Income[i], dat$Education[i], fitted_values[i], col = "red", lty = 2)
}


# Draw the slope lines
segments(0.37, 28500, 0.37, 30800, col = "purple", lty = 2)  # Horizontal line at slope_y1
segments(0.37, 28500, 0.324, 28500, col = "purple", lty = 2)   # Vertical line to slope_y2

# Annotate the lines
text(0.366, 29800, 'Slope', col = "purple", pos = 4, cex = 1)
text(0.369, 29000, expression(beta[1]), col = "purple", pos = 4, cex = 1.2)

```
]



---
# Building blocks of a linear regression model

.pull-left[
- **Slopes** ( $\beta_1$, $\beta_2$, ., $\beta_n$ ): These coefficients represent the change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='top', fig.width=3, fig.height=2}
par(mar = c(2, 2, 0.5, 2))  # Adjust bottom, left, top, and right margins
# Load the data:
data(trees)
names(trees) <- c("DBH_in","height_ft", "volume_ft3")
mod <- lm(DBH_in ~ height_ft, data = trees)
plot(DBH_in ~ height_ft, data = trees, pch=16)
abline(mod)

# Add general linear equation as expression
equation_text <- expression(Y == beta[0] + beta[1] * X)
text(x = 0.1, y = 34, labels = equation_text, pos = 4, col = "blue", cex = 1)

# Slope triangle
segments(0.32, 23, 0.32, 27, col = "purple", lty = 2)  # Vertical line for "changes in y"
segments(0.32, 23, 0.22, 23, col = "purple", lty = 2)  # Horizontal line for "1 unit of increase in x"

# Annotations for slope triangle
text(0.33, 25, "changes in Y", col = "purple", pos = 4, cex = 0.9)
text(0.27, 22.5, "1 unit of increase in X", col = "purple", pos = 1, cex = 0.9)
```
]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='top', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")


text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

# Draw dashed lines for residuals
residuals <- M$residuals
fitted_values <- fitted(M)

# Loop through the data points to draw dashed lines from each data point to the fitted line
for (i in 1:length(residuals)) {
  segments(dat$Education[i], dat$Income[i], dat$Education[i], fitted_values[i], col = "red", lty = 2)
}


# Draw the slope lines
segments(0.37, 28500, 0.37, 30800, col = "purple", lty = 2)  # Horizontal line at slope_y1
segments(0.37, 28500, 0.324, 28500, col = "purple", lty = 2)   # Vertical line to slope_y2

# Annotate the lines
text(0.366, 29800, 'Slope', col = "purple", pos = 4, cex = 1)
text(0.369, 29000, expression(beta[1]), col = "purple", pos = 4, cex = 1.2)

```
]


---
# Building blocks of a linear regression model

.pull-left[
- **Slopes** ( $\beta_1$, $\beta_2$, ., $\beta_n$ ): These coefficients represent the change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.

- The slope ( $\beta_1$ ) of your line:
  - It tells you how much will Y increase with each unit increase in X.
  - This is what you are interested in when examining trends over time (X).

]

.pull-right[
```{r , echo=FALSE, message=FALSE, warning=FALSE, fig.align='top', fig.width=5, fig.height=6}
# X represents the fraction of residents in each county having attained a
# bachelor's degree or better
x   <- c(0.19, 0.16, 0.40, 0.24, 0.31, 0.24, 0.28, 
         0.31, 0.18, 0.23, 0.17, 0.31, 0.15, 0.25, 
         0.19, 0.28)

y <- c(23663, 20659, 32277, 21595, 27227, 
       25023, 26504, 28741, 21735, 23366, 
       20871, 28370, 21105, 22706, 19527, 
       28321)

# Let's combine Income and Education into a single dataframe
dat <- data.frame(Income = y, Education = x)

# We will add county names to each row
row.names(dat) <- c("Androscoggin", "Aroostook", "Cumberland", "Franklin", "Hancock", "Kennebec", "Knox", "Lincoln", "Oxford", "Penobscot", "Piscataquis","Sagadahoc", "Somerset", "Waldo", "Washington", "York")

M <- lm( y ~ x, dat)

plot( y ~ x , dat, pch = 16, ylab=NA, mgp=c(4,1,.5) , las=1, bty="n", xaxs="i", xpd = TRUE,
      xlab="Fraction having attained a Bachelor's degree")
abline(M, col="blue")


text(0.28, 30000,  paste("Predicted value of y")) 
arrows(0.29,26900, 0.29,29500, lwd=1, arr.type="triangle")

# Add the regression equation as symbols (B + B1*X)
equation_text <- expression(Y == beta[0] + beta[1]*X)
# Position the equation text on the plot
text(x = 0.3, y = 32000, labels = equation_text, pos = 4, col = "red", cex = 1.2)

# Draw dashed lines for residuals
residuals <- M$residuals
fitted_values <- fitted(M)

# Loop through the data points to draw dashed lines from each data point to the fitted line
for (i in 1:length(residuals)) {
  segments(dat$Education[i], dat$Income[i], dat$Education[i], fitted_values[i], col = "red", lty = 2)
}


# Draw the slope lines
segments(0.37, 28500, 0.37, 30800, col = "purple", lty = 2)  # Horizontal line at slope_y1
segments(0.37, 28500, 0.324, 28500, col = "purple", lty = 2)   # Vertical line to slope_y2

# Annotate the lines
text(0.366, 29800, 'Slope', col = "purple", pos = 4, cex = 1)
text(0.369, 29000, expression(beta[1]), col = "purple", pos = 4, cex = 1.2)

```
]

---
# Explanation of Terms

$$y_i = \underbrace{\beta_0}_{\text{Intercept}} + \underbrace{\beta_1}_{\text{Slope}} x_i + \underbrace{\epsilon_i}_{\text{Error}}$$

---
# Explanation of Terms

$$y_i = \underbrace{\beta_0}_{\text{Intercept}} + \underbrace{\beta_1}_{\text{Slope}} x_i + \underbrace{\epsilon_i}_{\text{Error}}$$

- ** $y_i$ ** (Predicted Response Variable): The estimated or predicted value of the response variable.

- ** $\beta_0$ ** (Intercept): Represents the value of $y$ when $x$ is zero, or where it crosses the y axis

- ** $\beta_1$ **  (Slope): Represents the change in $y$ for a one-unit increase in $x$, showing the steepness and direction of the line.

- ** $x_i$ **  (Predictor Variable): The measured value of the independent variable.

- ** $\epsilon_i$ ** (Error Term): Accounts for variability in $y$ not explained by $x$, reflecting factors not captured in the model.
