---
title: "Hypothesis Testing, Probability and Distributions"
author: "Pablo E. GutiÃ©rrez-Fonseca"
date: 'Fall 2024'
tables: true
graphics: yes
output:   
  beamer_presentation:
    theme: "Boadilla"
    keep_tex: true
header-includes: 
  - \usepackage{graphicx}
  - \logo{\ifnum\thepage>1\hfill\includegraphics[width=1cm]{logo}\fi}
  - \titlegraphic{\includegraphics[width=3cm]{logo}}
  - \newcommand{\theHtable}{\thetable}
---



## Normality, Probability and Significance
- Why did we focus on normality?
  - The **normal distribution** is a key tool for determining the probability of a given value occurring in a population that follows this distribution.

<br>

- It allows us to make inferences about a population by calculating how likely it is for data to fall within certain ranges.

<br>

- Many statistical tests assume data follows a **normal distribution**, which helps in determining **significance** and making reliable conclusions.

## Normal distribution
- The normal distribution has two parameters (two numerical descriptive measures), the mean () and the standard deviation ().

<br>

- If X is a quantity to be measured that has a normal distribution with mean () and standard deviation (), we designate this by writing
X  N(,)
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=2.5}
library(ggplot2)
mean <- 100
sd <- 15
x <- seq(mean - 4*sd, mean + 4*sd, by = 0.1)
y <- dnorm(x, mean = mean, sd = sd)
ggplot(data.frame(x, y), aes(x = x, y = y)) +
  geom_line(color = "blue", size = 1.2) +
  labs(title = "", x = "Values", y = "Density")
```

## Hypothesis Testing 
- All inferential tests use a formula that calculates a **test statistic**, quantifying the relationship or difference you are testing.

::: columns
:::: column
\small
- Independent t-test:
```{r echo=FALSE, results='asis'}
cat("$$t = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$")

```

  - Dependent t-test
```{r echo=FALSE, results='asis'}
cat("$$t = \\frac{\\bar{D}}{\\frac{s_D}{\\sqrt{n}}}$$")
```

  - One sample z-test
```{r echo=FALSE, results='asis'}
cat("$$z = \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}$$")
```
\small  
::::

:::: column
\small
- F-test (ANOVA)
```{r echo=FALSE, results='asis'} 
cat("$$F = \\frac{MST}{MSE}$$")
```

- Pearson correlation
```{r echo=FALSE, results='asis'} 
cat("$$r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}$$")
```
\
::::
:::


## Hypothesis Testing 
- We use the **normal distribution curve** to find the **probability** of obtaining a **test statistic** as extreme as the one observed, assuming the **Null Hypothesis is true**.


## Hypothesis Testing 
- If the **probability** (*_the area under the curve_*) of obtaining a test statistic that extreme is less than our chosen significance threshold (*_usually 0.05_*), we conclude that the result is **statistically significant**.




## Hypothesis Testing 
- Another way to think about this:
  - If the probability that our data **fits the null distribution (i.e., the null hypothesis is true) is less than 5%**, then we conclude that the data **does not fit the null**.
      - **Significant** deviation from what is expected by chance.
      - **Reject** the null hypothesis.
      - We have a **significant result**. 

## Why p-value of less than **0.05**?

::: columns

:::: column

“It is usual and convenient for experimenters to take 5% as a standard level of significance, in the sense that they are prepared to ignore all results which fail to reach this standard, and, by this means, to eliminate from further discussion the greater part of the fluctuations which chance causes have introduced into their experimental results.”

::::

:::: column
- Ronald Aylmer Fisher (1890-1962)

<br>

```{r, echo=FALSE, fig.cap="", out.width = '80%'}
knitr::include_graphics("figs/fisher.png")
```
::::

:::


## What is Significance?
- A statistical result is **significant** if it is **unlikely to have occurred by chance**.



- Even though there is natural variability in the population, the probability of observing a value this extreme due to random variability is **low** (though not impossible).

<br>

- We use probabilities (**p-values**) and an **alpha threshold (commonly 0.05)** to determine whether a result is significant.

## What is Significance?
- Significance refers to the risk of rejecting the null hypothesis when it is actually true.

<br>

- It tells us the probability that our result happened by chance alone.

<br>

- A p-value of 0.05 (5%) means there's a 5% chance the result is due to random chance.

<br>

- A p-value of 0.01 (1%) means there's a 1% chance the result happened by chance.



## Calculating Significance
- To quantify a probability, you first need to calculate a **test statistic** and locate it on the normal probability curve.

- The normal curve acts as a statistical translator.
    - It helps you standardize your test statistic to a common scale.
    - This standardized value is then used to determine the probability of obtaining such a result in a standard normal population.



## Z-score

- **Z-scores** link measured or hypothesized values to probabilities.

<br>

- A Z-score (or standard score) indicates how many standard deviations a value *_x_* is above or below the mean on the normal curve.

<br>

- It helps standardize values and connect them to probabilities.

- The Z-score is calculated using the formula:

\[ Z = \frac{(x - \mu)}{\sigma} \]

- where:  
  - \( x \) = the value of interest.  
  - \( \mu \) = mean of the population.  
  - \( \sigma \) = standard deviation of the population.  

## Z-Scores: Linking Observations to Probabilities

- **Z-scores** link observations to probabilities.
- Using a Z-score for probability:
  - Z-scores are essentially the **x-axis** of the standard normal distribution.
  - They normalize any data set so that the mean is 0 and the standard deviation is 1.
  - The area under the curve tells you the probability of a certain Z-score occurring.
  - By using the Z-score, we can determine the probability associated with different values.


## Finding Probabilities for Z-Scores
 
::: columns


:::: column
\vspace{1cm}

- **P(X < z)**: Denotes the probability of a value falling **less than** a given Z-score (\( z \)).

\vspace{1cm}

- **P(X > z)**: Denotes the probability of a value falling **above** a given Z-score (\( z \)).

\vspace{1cm}

- **P(z1 < X < z2)**: Denotes the probability of a value falling **between** two different Z-scores (\( z1 \) and \( z2 \)).

::::

:::: column
```{r, echo=FALSE, message=FALSE, fig.cap="", fig.width=3, fig.height=4.5}
library(ggplot2)
library(dplyr)

# Create a data frame for the standard normal distribution
df <- data.frame(x = seq(-4, 4, length.out = 1000))
df$y <- dnorm(df$x)

# Plot 1: P(X < z)
z1 <- 1  # Example Z-score
p1 <- ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = subset(df, x < z1), aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = z1, linetype = "dashed", color = "red") +
  labs(title = "P(X < z)", x = "Z-score", y = "Density") +
  theme_minimal()

# Plot 2: P(X > z)
z2 <- 1  # Example Z-score
p2 <- ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = subset(df, x > z2), aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = z2, linetype = "dashed", color = "red") +
  labs(title = "P(X > z)", x = "Z-score", y = "Density") +
  theme_minimal()

# Plot 3: P(z1 < X < z2)
z1 <- -1  # Example Z-score 1
z2 <- 1    # Example Z-score 2
p3 <- ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = subset(df, x > z1 & x < z2), aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = c(z1, z2), linetype = "dashed", color = "red") +
  labs(title = "P(z1 < X < z2)", x = "Z-score", y = "Density") +
  theme_minimal()

# Arrange the plots in a grid
library(gridExtra)
grid.arrange(p1, p2, p3, ncol = 1)
```
::::

:::


## Example


## Example: Probability of a Cat Living to a Certain Age
- The lifespan of domestic cats is normally distributed with a mean of 15.7 years and a standard deviation of 1.6 years. 

::: columns

:::: column
\vspace{1cm}
- **Question**: What is the probability that a cat will live to be as old as Allison’s 18-year-old cat?

<br>

- We're looking for the probability P(X > 18), which represents the probability that a cat will live longer than 18 years.

::::

:::: column
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="", fig.width=3, fig.height=3.5}
# Load necessary libraries
library(ggplot2)

# Given values
mean_lifespan <- 15.7
sd_lifespan <- 1.6
age_allison_cat <- 18

# Define the Z-score
z_score <- (age_allison_cat - mean_lifespan) / sd_lifespan

# Generate a sequence of x-values (ages) covering the normal distribution
x_values <- seq(mean_lifespan - 4 * sd_lifespan, mean_lifespan + 4 * sd_lifespan, length = 1000)

# Calculate the density for each x-value
density <- dnorm(x_values, mean = mean_lifespan, sd = sd_lifespan)

# Create a data frame for ggplot
df <- data.frame(x = x_values, density = density)

# Define the critical Z-score for shading
shade_df <- df[df$x >= age_allison_cat, ]

# Plot the normal distribution with the shaded area for P(X > 18)
ggplot(df, aes(x = x, y = density)) +
  geom_line(color = "blue", size = 1) +  # Plot the normal distribution
  geom_area(data = shade_df, aes(x = x, y = density), fill = "red", alpha = 0.4) +  # Shade the area P(X > 18)
  geom_vline(xintercept = age_allison_cat, linetype = "dashed", color = "black") +  # Add a dashed line at Allison's cat's age
  labs(title = "Probability of a Cat Living to be as Old as Allison's Cat",
       subtitle = paste0("Mean = 15.7, SD = 1.6, Age = ", age_allison_cat),
       x = "Cat's Age",
       y = "Density") +
  theme_minimal()
```

::::

:::


## **Steps 1**:
-  **Calculate the Z-score**:

::: columns
:::: column

   \[
   Z = \frac{X - \mu}{\sigma}
   \]

- where:
   - \( X \) is the value (18 years),
   - \( \mu \) is the mean (15.7 years),
   - \( \sigma \) is the standard deviation (1.6 years).

::::

:::: column

\vspace{1cm}

   \[
   Z = \frac{18 - 15.7}{1.6} = \frac{2.3}{1.6} \approx 1.4375
   \]

::::

:::



## Step 2. 
- Find the Probability:

```{r}
# Given values
mean_lifespan <- 15.7
sd_lifespan <- 1.6
age_allison_cat <- 18
# Calculate Z-score
z_score <- (age_allison_cat - mean_lifespan) / sd_lifespan
# Find the probability that a cat lives longer than 18 years
probability <- 1 - pnorm(z_score)
probability
```

- Thus, the probability that a cat will live to be as old as or older than 18 years is approximately **0.0749** or **7.49%**.



## Step 2. 
- Find the Probability:

```{r}
# Given values
mean_lifespan <- 15.7
sd_lifespan <- 1.6
age_allison_cat <- 18
# Calculate Z-score
z_score <- (age_allison_cat - mean_lifespan) / sd_lifespan
# Find the probability that a cat lives longer than 18 years
probability <- 1 - pnorm(z_score)
probability
```

- This example walks you through calculating the probability in R using `pnorm()`, which calculates the cumulative probability under the normal distribution.







## What is a P-Value?
- **Definition**: The p-value is the probability of obtaining a result at least as extreme as the one observed, assuming the null hypothesis is true.

<br>

- In Simple Terms, it tells us how likely it is to see the data we have (or something more extreme) if there were actually no effect or difference.

<br>

- Interpretation:

  - A small p-value indicates that the observed result is unlikely under the null hypothesis, suggesting evidence against it.  
  
<br>

  - A large p-value suggests that the observed result is consistent with the null hypothesis.
  
##

## Hypothesis Testing Summary

| Scenario                      | Null Hypothesis is True                                         | Null Hypothesis is False                                          |
|-------------------------------|-----------------------------------------------------------------|--------------------------------------------------------------------|
| **Reject Null Hypothesis**    | **Type I Error**: Incorrectly rejecting the null hypothesis.    | **Correct Decision**: Correctly rejecting the null hypothesis.    |
| **Fail to Reject Null Hypothesis** | **Correct Decision**: Correctly not rejecting the null hypothesis. | **Type II Error**: Incorrectly failing to reject the null hypothesis. |


## Hypothesis Testing Summary

| Scenario                      | Null Hypothesis is True                                         | Null Hypothesis is False                                          |
|-------------------------------|-----------------------------------------------------------------|--------------------------------------------------------------------|
| **Reject Null Hypothesis**    | **Type I Error**: Incorrectly rejecting the null hypothesis.    | **Correct Decision**: Correctly rejecting the null hypothesis.    |
| **Fail to Reject Null Hypothesis** | **Correct Decision**: Correctly not rejecting the null hypothesis. | **Type II Error**: Incorrectly failing to reject the null hypothesis. |

<br> 
\small
- Failing to reject the null hypothesis means our data didn't show a significant effect. It doesn't prove the null hypothesis is true; it just means we didn't find strong evidence against it.
\

## 
- **Explanation**:  

  - **Type I Error**: False positive. We conclude there is an effect or difference when there is none.  

<br>

  - **Type II Error**: False negative. We fail to detect an effect or difference when one exists.

<br>

  - **Correct Decisions**: Accurately concluding the presence or absence of an effect or difference based on the truth of the null hypothesis.


##  
