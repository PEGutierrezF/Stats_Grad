---
title: "Hypothesis Testing, Probability and Distributions"
author: "Pablo E. Gutiérrez-Fonseca"
date: 'Fall 2024'
tables: true
graphics: yes
output:   
  beamer_presentation:
    theme: "Boadilla"
    keep_tex: true
header-includes: 
  - \usepackage{graphicx}
  - \logo{\ifnum\thepage>1\hfill\includegraphics[width=1cm]{logo}\fi}
  - \titlegraphic{\includegraphics[width=3cm]{logo}}
---

## Normality, Probability and Significance
- Why did we focus on normality?
  - The **normal distribution** is a key tool for determining the probability of a given value occurring in a population that follows this distribution.

<br>

- It allows us to make inferences about a population by calculating how likely it is for data to fall within certain ranges.

<br>

- Many statistical tests assume data follows a **normal distribution**, which helps in determining **significance** and making reliable conclusions.


## Hypothesis Testing 
- All inferential tests use a formula that calculates a **test statistic**, quantifying the relationship or difference you are testing.


## Hypothesis Testing 
- We use the **normal distribution curve** to find the **probability** of obtaining a **test statistic** as extreme as the one observed, assuming the **Null Hypothesis is true**.


## Hypothesis Testing 
- If the **probability** (*_the area under the curve_*) of obtaining a test statistic that extreme is less than our chosen significance threshold (*_usually 0.05_*), we conclude that the result is **statistically significant**.


## Hypothesis Testing 
- Another way to think about this:
  - If the probability that our data **fits the null distribution (i.e., the null hypothesis is true) is less than 5%**, then we conclude that the data **does not fit the null**.
      - **Significant** deviation from what is expected by chance.
      - **Reject** the null hypothesis.
      - We have a **significant result**. 

## Why p-value of less than **0.05**?

::: columns

:::: column

“It is usual and convenient for experimenters to take 5% as a standard level of significance, in the sense that they are prepared to ignore all results which fail to reach this standard, and, by this means, to eliminate from further discussion the greater part of the fluctuations which chance causes have introduced into their experimental results.”

::::

:::: column
- Ronald Aylmer Fisher (1890-1962)

<br>

```{r, echo=FALSE, fig.cap="", out.width = '80%'}
knitr::include_graphics("figs/fisher.png")
```
::::

:::


## What is Significance?
- A statistical result is **significant** if it is **unlikely to have occurred by chance**.



- Even though there is natural variability in the population, the probability of observing a value this extreme due to random variability is **low** (though not impossible).

<br>

- We use probabilities (**p-values**) and an **alpha threshold (commonly 0.05)** to determine whether a result is significant.

## What is Significance?
- Significance refers to the risk of rejecting the null hypothesis when it is actually true.

<br>

- It tells us the probability that our result happened by chance alone.

<br>

- A p-value of 0.05 (5%) means there's a 5% chance the result is due to random chance.

<br>

- A p-value of 0.01 (1%) means there's a 1% chance the result happened by chance.



## Calculating Significance
- To quantify a probability, you first need to calculate a **test statistic** and locate it on the normal probability curve.

- The normal curve acts as a statistical translator.
    - It helps you standardize your test statistic to a common scale.
    - This standardized value is then used to determine the probability of obtaining such a result in a standard normal population.



## Z-score

- **Z-scores** link measured or hypothesized values to probabilities.

<br>

- A Z-score (or standard score) indicates how many standard deviations a value *_x_* is above or below the mean on the normal curve.

<br>

- It helps standardize values and connect them to probabilities.

- The Z-score is calculated using the formula:

\[ Z = \frac{(x - \mu)}{\sigma} \]

- where:  
  - \( x \) = the value of interest.  
  - \( \mu \) = mean of the population.  
  - \( \sigma \) = standard deviation of the population.  

## Z-Scores: Linking Observations to Probabilities

- **Z-scores** link observations to probabilities.
- Using a Z-score for probability:
  - Z-scores are essentially the **x-axis** of the standard normal distribution.
  - They normalize any data set so that the mean is 0 and the standard deviation is 1.
  - The area under the curve tells you the probability of a certain Z-score occurring.
  - By using the Z-score, we can determine the probability associated with different values.


## Finding Probabilities for Z-Scores

::: columns

:::: column
- **P(X < z)**: Denotes the probability of a value falling **less than** a given Z-score (\( z \)).

<br>

- **P(X > z)**: Denotes the probability of a value falling **above** a given Z-score (\( z \)).

<br>

- **P(z1 < X < z2)**: Denotes the probability of a value falling **between** two different Z-scores (\( z1 \) and \( z2 \)).

::::

:::: column

```{r, echo=FALSE, message=FALSE, fig.cap="", fig.width=3, fig.height=5}
library(ggplot2)
library(dplyr)

# Create a data frame for the standard normal distribution
df <- data.frame(x = seq(-4, 4, length.out = 1000))
df$y <- dnorm(df$x)

# Plot 1: P(X < z)
z1 <- 1  # Example Z-score
p1 <- ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = subset(df, x < z1), aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = z1, linetype = "dashed", color = "red") +
  labs(title = "P(X < z)", x = "Z-score", y = "Density") +
  theme_minimal()

# Plot 2: P(X > z)
z2 <- 1  # Example Z-score
p2 <- ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = subset(df, x > z2), aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = z2, linetype = "dashed", color = "red") +
  labs(title = "P(X > z)", x = "Z-score", y = "Density") +
  theme_minimal()

# Plot 3: P(z1 < X < z2)
z1 <- -1  # Example Z-score 1
z2 <- 1    # Example Z-score 2
p3 <- ggplot(df, aes(x = x, y = y)) +
  geom_line() +
  geom_area(data = subset(df, x > z1 & x < z2), aes(x = x, y = y), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = c(z1, z2), linetype = "dashed", color = "red") +
  labs(title = "P(z1 < X < z2)", x = "Z-score", y = "Density") +
  theme_minimal()

# Arrange the plots in a grid
library(gridExtra)
grid.arrange(p1, p2, p3, ncol = 1)
```
::::

:::

## What is a P-Value?
- **Definition**: The p-value is the probability of obtaining a result at least as extreme as the one observed, assuming the null hypothesis is true.

<br>

- In Simple Terms, it tells us how likely it is to see the data we have (or something more extreme) if there were actually no effect or difference.

<br>

- Interpretation:

  - A small p-value indicates that the observed result is unlikely under the null hypothesis, suggesting evidence against it.  
  - A large p-value suggests that the observed result is consistent with the null hypothesis.
  
  
